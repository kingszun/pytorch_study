{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch.Autograd를 이용한 자동미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망을 학습할 때 가장 자주 사용되는 알고리즘은 역전파입니다. 이 알고리즘에서, 매개변수(모델 가중치)는 주어진 매개변수에 대한 손실 함수의 변화도(gradient)에 따라 조정됩니다.\n",
    "\n",
    "이러한 변화도를 계산하기 위해 PyTorch에는 torch.autograd라고 불리는 자동 미분 엔진이 내장되어 있습니다. 이는 모든 계산 그래프에 대한 변화도의 자동 계산을 지원합니다.\n",
    "\n",
    "입력 x, 매개변수 w와 b , 그리고 일부 손실 함수가 있는 가장 간단한 단일 계층 신경망을 가정하겠습니다. PyTorch에서는 다음과 같이 정의할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) x tensor([[ 0.8557,  1.0269,  0.6058],\n",
      "        [-0.1978, -1.1899, -1.0107],\n",
      "        [-0.1256,  0.1268, -1.0994],\n",
      "        [ 1.3848,  0.1957, -0.2494],\n",
      "        [ 0.7276, -0.1655,  0.5424]], requires_grad=True) + tensor([ 0.6087, -0.1200, -0.0736], requires_grad=True) = tensor([ 3.2533, -0.1260, -1.2849], grad_fn=<AddBackward0>)\n",
      "1.38919198513031 = torch.nn.functional.binary_cross_entropy_with_logits(tensor([ 3.2533, -0.1260, -1.2849], grad_fn=<AddBackward0>), tensor([0., 0., 0.]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # output tensor\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(f\"{x} x {w} + {b} = {z}\")\n",
    "print(f\"{loss} = torch.nn.functional.binary_cross_entropy_with_logits({z}, {y})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x1077feac0>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x1077fec40>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "tensor([[0.3209, 0.1562, 0.0722],\n",
      "        [0.3209, 0.1562, 0.0722],\n",
      "        [0.3209, 0.1562, 0.0722],\n",
      "        [0.3209, 0.1562, 0.0722],\n",
      "        [0.3209, 0.1562, 0.0722]])\n",
      "tensor([0.3209, 0.1562, 0.0722])\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)\n",
    "print(f\"{loss.backward()}\")\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변화도 추적 멈추기\n",
    "Requires_grad=True인 모든 텐서는 연산 기록을 추적하고 Gradient 계산을 지원함.   \n",
    "그러나 모델 학습 후 inference 단계에서는 forward propagation만 하면 됨.   \n",
    "이럴경우 연산 코드를 ```torch.no_grad```블록으로 둘러싸서 연산을 멈출 수 있음.   \n",
    "또는 ```.detach()```메소드를 사용해서 연산을 멈출 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 순전파 단계\n",
    "  - 요청된 연산을 수행하여 텐서를 계산\n",
    "  - DAG(Directed Acyclic Graph)에 연산의 Gradient function을 maintain한다.\n",
    "- 역전파 단계\n",
    "  - DAG의 Root에서 .backward가 호출될 때 시작된다.\n",
    "  - 각 .grad_fn으로 gradient계산.\n",
    "  - 각 tensor.grad 속성에 계산 결과를 accumulate.\n",
    "  - 연쇄 법칙을 사용해서 모든 leaf tensor들까지 propagate 한다.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z - torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n",
    "z_det = z.detach_()\n",
    "print(z.requires_grad)\n",
    "print(z_det.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w) + b\n",
    "print(z.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor.pow(n): 각 인수에 n제곱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]], requires_grad=True)\n",
      "out\n",
      " tensor([[4., 1., 1., 1., 1.],\n",
      "        [1., 4., 1., 1., 1.],\n",
      "        [1., 1., 4., 1., 1.],\n",
      "        [1., 1., 1., 4., 1.],\n",
      "        [1., 1., 1., 1., 4.]], grad_fn=<PowBackward0>)\n",
      "torch.ones_like(inp): \n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.],\n",
      "        [4., 4., 4., 4., 8.]])\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.],\n",
      "        [2., 2., 2., 2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "inp = torch.eye(5, requires_grad=True)\n",
    "out = (inp+1).pow(2)\n",
    "print(f\"in\\b{inp}\")\n",
    "print(f\"out\\n {out}\")\n",
    "print(f\"torch.ones_like(inp): \\n{torch.ones_like(inp)}\")\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"Second call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(inp), retain_graph=True)\n",
    "print(f\"Call after zeroing gradients\\n{inp.grad}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88ed026e6e6df446360fa5f524e4e892f0492e13534613446d365f4e42b49d96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
